segments$Cluster <- as.factor(clusters)
head(segments, 10)
summary(cw_data)
unique_countries <- unique(cw_data$Country)
print(unique_countries)
sales_by_country <- cw_data %>%
group_by(Country) %>%
summarise(total_sales = sum(UnitPrice*Quantity))
sales_by_country
sales_by_country <- sales_by_country %>%
mutate(Country = reorder(Country, -total_sales))
ggplot(sales_by_country, aes(x = Country, y = total_sales, fill = Country)) +
geom_bar(stat = "identity", color = "black") +
labs(title = "Total Sales by Country", x = "Country", y = "Total Sales") +
theme_minimal() +
theme(axis.text.x = element_text(size=8,angle = 90, hjust = 1),
axis.title.x = element_text(size = 10),
plot.title = element_text(size = 18, hjust=0.5, face = 'bold'),
legend.text = element_text(size = 6),
legend.title = element_text(size = 6),
legend.position = "top",
legend.key.size = unit(0.3, "cm"))+
scale_y_continuous(labels = scales::comma_format(scale = 1e-6, suffix = "M"))
ggplot(sales_by_country, aes(x = Country, y = total_sales, fill = Country)) +
geom_bar(stat = "identity", color = "black") +
labs(title = "Total Sales by Country", x = "Country", y = "Total Sales (log scale)") +
theme_minimal() +
theme(axis.text.x = element_text(size=8, angle = 90, hjust = 1),
axis.title.x = element_text(size = 10),
plot.title = element_text(size = 18, hjust=0.5, face = 'bold'),
legend.text = element_text(size = 6),
legend.title = element_text(size = 6),
legend.position = "top",
legend.key.size = unit(0.3, "cm")) +
scale_y_log10(labels = scales::comma_format(scale = 1e-6, suffix = "M"))
ggplot(cw_data, aes(x = reorder(Country, Quantity, FUN = median), y = Quantity)) +
geom_boxplot( color = "royalblue", alpha = 0.7) +
labs(title = "Order Quantity Distribution by Country", x = "Country", y = "Order Quantity (in units)") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
plot.title = element_text(size = 18, hjust=0.5, face = 'bold'),
legend.position = "none")
cancelled_data <- cw_data %>%
filter(startsWith(InvoiceNo, "C"))
sales_plot <- cw_data %>%
group_by(InvoiceDate) %>%
summarise(
TotalSales = sum(Quantity * UnitPrice),
cancelledSales = sum(ifelse(startsWith(InvoiceNo, "C"), Quantity * UnitPrice, 0))
)
ggplot(sales_plot, aes(x = InvoiceDate)) +
geom_line(aes(y = TotalSales, color = "Total Sales"), size = 1.2, alpha = 0.8) +
geom_line(aes(y = cancelledSales, color = "cancelled Sales"), size = 1.2, alpha = 0.8, linetype = "dashed") +
labs(title = "Sales Dynamics: Total vs Cancelled",
x = "Date",
y = "Sales",
color = "Sales Type") +
scale_y_continuous(labels = scales::comma_format()) +
theme_minimal() +
theme(legend.position = "top", plot.title = element_text(size = 18, hjust=0.5, face = 'bold'),
legend.title = element_blank())
cw_data <- read.csv("Online Retail.csv") #the data set is assigned to the variable cw_data
head(cw_data)
summary(cw_data)
library(dplyr) #dplyr library is essentially a set of functions designed to make manipulating dataframes more intuitive and user-friendly.
missing_values_per_column <- colSums(is.na(cw_data))
cat("Missing Values Per Column:\n")
print(missing_values_per_column)
library(lubridate) #lubridate package offers utilities that make date-time information manipulation, extraction, and computation easier.
cw_data$InvoiceDate <- as.POSIXct(cw_data$InvoiceDate, format = "%d/%m/%Y %H:%M")
sum(is.na(cw_data$InvoiceDate))
product_sales <- aggregate(cw_data$Quantity, by = list(Product = cw_data$Description), sum)
productlist_sorted <- product_sales[order(-product_sales$x), ]
head(productlist_sorted,10)
cat("The Most Popular Product is", productlist_sorted$Product[1],"\n")
library(ggplot2)
productlist_top3 <- cw_data %>%
filter(Description %in% c("WORLD WAR 2 GLIDERS ASSTD DESIGNS", "JUMBO BAG RED RETROSPOT", "ASSORTED COLOUR BIRD ORNAMENT")) %>%
mutate(total_sales = Quantity * UnitPrice) #the top 3 out of the top 5 products from the above list is added in this
productlist_top3 <- productlist_top3 %>%
filter(total_sales >= 0)
ggplot(productlist_top3, aes(x = InvoiceDate, y = Quantity, color = Description)) +
geom_point(size = 2, alpha = 0.8, show.legend = TRUE) +
geom_line(size = 1.5, alpha = 0.8, linetype = "solid") +
labs(title = "Top 3 Products: Sales Over Time", x = "Date", y = "Quantity") +
theme_minimal() +
scale_color_brewer(palette = "Set1") +
theme(legend.position = "top",
legend.text = element_text(size = 7),
legend.title = element_blank(),
axis.text.x = element_text(angle = 45, hjust = 1)) +
annotate("text", x = max(productlist_top3$InvoiceDate),
y = max(productlist_top3$total_sales),
label = "",
hjust = 1, vjust = 1, size = 3, color = "gray50") +
scale_x_datetime(date_labels = "%b %Y", date_breaks = "3 months") +
theme(plot.title = element_text(hjust = 0.5, size = 18, face = "bold"),
plot.caption = element_text(size = 8, color="gray50"))
filtered_data <- cw_data %>%
filter(CustomerID != 0) #to filter out rows where CustomerID is equal to 0
head(filtered_data)
nrow(filtered_data) #nrow() is used to check the total number of rows in the data set
segments <- cw_data %>%
group_by(CustomerID) %>%
summarise(
TotalPurchases = n(),
TotalSpending = sum(Quantity * UnitPrice)
)
head(segments)
nrow(segments)
clustering_data <- segments[, c("TotalPurchases", "TotalSpending")]
hc <- hclust(dist(clustering_data))
clusters <- cutree(hc, k = 5)
segments$Cluster <- as.factor(clusters)
head(segments, 10)
summary(cw_data)
unique_countries <- unique(cw_data$Country)
print(unique_countries)
sales_by_country <- cw_data %>%
group_by(Country) %>%
summarise(total_sales = sum(UnitPrice*Quantity))
sales_by_country
sales_by_country <- sales_by_country %>%
mutate(Country = reorder(Country, -total_sales))
ggplot(sales_by_country, aes(x = Country, y = total_sales, fill = Country)) +
geom_bar(stat = "identity", color = "black") +
labs(title = "Total Sales by Country", x = "Country", y = "Total Sales") +
theme_minimal() +
theme(axis.text.x = element_text(size=8,angle = 90, hjust = 1),
axis.title.x = element_text(size = 10),
plot.title = element_text(size = 18, hjust=0.5, face = 'bold'),
legend.text = element_text(size = 6),
legend.title = element_text(size = 6),
legend.position = "top",
legend.key.size = unit(0.3, "cm"))+
scale_y_continuous(labels = scales::comma_format(scale = 1e-6, suffix = "M"))
ggplot(sales_by_country, aes(x = Country, y = total_sales, fill = Country)) +
geom_bar(stat = "identity", color = "black") +
labs(title = "Total Sales by Country(Logarithmic Scale)", x = "Country", y = "Total Sales (log scale)") +
theme_minimal() +
theme(axis.text.x = element_text(size=8, angle = 90, hjust = 1),
axis.title.x = element_text(size = 10),
plot.title = element_text(size = 18, hjust=0.5, face = 'bold'),
legend.text = element_text(size = 6),
legend.title = element_text(size = 6),
legend.position = "top",
legend.key.size = unit(0.3, "cm")) +
scale_y_log10(labels = scales::comma_format(scale = 1e-6, suffix = "M"))
cw_data <- read.csv("Online Retail.csv") #the data set is assigned to the variable cw_data
head(cw_data)
summary(cw_data)
library(dplyr) #dplyr library is essentially a set of functions designed to make manipulating dataframes more intuitive and user-friendly.
missing_values_per_column <- colSums(is.na(cw_data))
cat("Missing Values Per Column:\n")
print(missing_values_per_column)
library(lubridate) #lubridate package offers utilities that make date-time information manipulation, extraction, and computation easier.
cw_data$InvoiceDate <- as.POSIXct(cw_data$InvoiceDate, format = "%d/%m/%Y %H:%M")
sum(is.na(cw_data$InvoiceDate))
product_sales <- aggregate(cw_data$Quantity, by = list(Product = cw_data$Description), sum)
productlist_sorted <- product_sales[order(-product_sales$x), ]
head(productlist_sorted,10)
cat("The Most Popular Product is", productlist_sorted$Product[1],"\n")
library(ggplot2)
productlist_top3 <- cw_data %>%
filter(Description %in% c("WORLD WAR 2 GLIDERS ASSTD DESIGNS", "JUMBO BAG RED RETROSPOT", "ASSORTED COLOUR BIRD ORNAMENT")) %>%
mutate(total_sales = Quantity * UnitPrice) #the top 3 out of the top 5 products from the above list is added in this
productlist_top3 <- productlist_top3 %>%
filter(total_sales >= 0)
ggplot(productlist_top3, aes(x = InvoiceDate, y = Quantity, color = Description)) +
geom_point(size = 2, alpha = 0.8, show.legend = TRUE) +
geom_line(size = 1.5, alpha = 0.8, linetype = "solid") +
labs(title = "Top 3 Products: Sales Over Time", x = "Date", y = "Quantity") +
theme_minimal() +
scale_color_brewer(palette = "Set1") +
theme(legend.position = "top",
legend.text = element_text(size = 7),
legend.title = element_blank(),
axis.text.x = element_text(angle = 45, hjust = 1)) +
annotate("text", x = max(productlist_top3$InvoiceDate),
y = max(productlist_top3$total_sales),
label = "",
hjust = 1, vjust = 1, size = 3, color = "gray50") +
scale_x_datetime(date_labels = "%b %Y", date_breaks = "3 months") +
theme(plot.title = element_text(hjust = 0.5, size = 18, face = "bold"),
plot.caption = element_text(size = 8, color="gray50"))
filtered_data <- cw_data %>%
filter(CustomerID != 0) #to filter out rows where CustomerID is equal to 0
head(filtered_data)
nrow(filtered_data) #nrow() is used to check the total number of rows in the data set
segments <- cw_data %>%
group_by(CustomerID) %>%
summarise(
TotalPurchases = n(),
TotalSpending = sum(Quantity * UnitPrice)
)
head(segments)
nrow(segments)
clustering_data <- segments[, c("TotalPurchases", "TotalSpending")]
hc <- hclust(dist(clustering_data))
clusters <- cutree(hc, k = 5)
segments$Cluster <- as.factor(clusters)
head(segments, 10)
summary(cw_data)
unique_countries <- unique(cw_data$Country)
print(unique_countries)
sales_by_country <- cw_data %>%
group_by(Country) %>%
summarise(total_sales = sum(UnitPrice*Quantity))
sales_by_country
sales_by_country <- sales_by_country %>%
mutate(Country = reorder(Country, -total_sales))
ggplot(sales_by_country, aes(x = Country, y = total_sales, fill = Country)) +
geom_bar(stat = "identity", color = "black") +
labs(title = "Total Sales by Country", x = "Country", y = "Total Sales") +
theme_minimal() +
theme(axis.text.x = element_text(size=8,angle = 90, hjust = 1),
axis.title.x = element_text(size = 10),
plot.title = element_text(size = 18, hjust=0.5, face = 'bold'),
legend.text = element_text(size = 6),
legend.title = element_text(size = 6),
legend.position = "top",
legend.key.size = unit(0.3, "cm"))+
scale_y_continuous(labels = scales::comma_format(scale = 1e-6, suffix = "M"))
ggplot(sales_by_country, aes(x = Country, y = total_sales, fill = Country)) +
geom_bar(stat = "identity", color = "black") +
labs(title = "Total Sales by Country(Logarithmic Scale)", x = "Country", y = "Total Sales (log scale)") +
theme_minimal() +
theme(axis.text.x = element_text(size=8, angle = 90, hjust = 1),
axis.title.x = element_text(size = 10),
plot.title = element_text(size = 18, hjust=0.5, face = 'bold'),
legend.text = element_text(size = 6),
legend.title = element_text(size = 6),
legend.position = "top",
legend.key.size = unit(0.3, "cm")) +
scale_y_log10(labels = scales::comma_format(scale = 1e-6, suffix = "M"))
ggplot(cw_data, aes(x = reorder(Country, Quantity, FUN = median), y = Quantity)) +
geom_boxplot( color = "royalblue", alpha = 0.7) +
labs(title = "Order Quantity Distribution by Country", x = "Country", y = "Order Quantity (in units)") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
plot.title = element_text(size = 18, hjust=0.5, face = 'bold'),
legend.position = "none")
cancelled_data <- cw_data %>%
filter(startsWith(InvoiceNo, "C"))
sales_plot <- cw_data %>%
group_by(InvoiceDate) %>%
summarise(
TotalSales = sum(Quantity * UnitPrice),
cancelledSales = sum(ifelse(startsWith(InvoiceNo, "C"), Quantity * UnitPrice, 0))
)
ggplot(sales_plot, aes(x = InvoiceDate)) +
geom_line(aes(y = TotalSales, color = "Total Sales"), size = 1.2, alpha = 0.8) +
geom_line(aes(y = cancelledSales, color = "cancelled Sales"), size = 1.2, alpha = 0.8, linetype = "dashed") +
labs(title = "Sales Dynamics: Total vs Cancelled",
x = "Date",
y = "Sales",
color = "Sales Type") +
scale_y_continuous(labels = scales::comma_format()) +
theme_minimal() +
theme(legend.position = "top", plot.title = element_text(size = 18, hjust=0.5, face = 'bold'),
legend.title = element_blank())
#Part 1 Manual Modelling
#Setting the working directory
setwd("C:/Users/Ritwik Singh/OneDrive - University of Bath/Documents/Semester 1/Business Statistics and Forecasting/Coursework")
#Reading of the CSV data
sanjali_data <- read.csv("MN50751CourseworkData2023.csv")
#Part 1 Manual Modelling
#Setting the working directory
setwd("C:/Users/Ritwik Singh/OneDrive - University of Bath/Documents/Semester 1/Business Statistics and Forecasting/Coursework")
#Reading of the CSV data
sanjali_data <- read.csv("MN50751CourseworkData2023.csv")
# Load necessary libraries
library(forecast)
library(smooth)
install.packages("smooth")
#Part 1 Manual Modelling
#Setting the working directory
setwd("C:/Users/Ritwik Singh/OneDrive - University of Bath/Documents/Semester 1/Business Statistics and Forecasting/Coursework")
#Reading of the CSV data
sanjali_data <- read.csv("MN50751CourseworkData2023.csv")
# Load necessary libraries
library(forecast)
library(smooth)
library(Mcomp)
library(tseries)
library(parallel)
library(foreach)
library(doSNOW)
install.packages("dosnow")
yes
install.packages("doSNOW")
#Part 1 Manual Modelling
#Setting the working directory
setwd("C:/Users/Ritwik Singh/OneDrive - University of Bath/Documents/Semester 1/Business Statistics and Forecasting/Coursework")
#Reading of the CSV data
sanjali_data <- read.csv("MN50751CourseworkData2023.csv")
# Load necessary libraries
library(forecast)
library(smooth)
library(Mcomp)
library(tseries)
library(parallel)
library(foreach)
library(doSNOW)
library(dplyr)
# Set your Student ID last digit
student_number_end_digit <- 30
#Define cluster
cores <- detectCores()
cl <- makeCluster(cores - 1, type = "SOCK")
registerDoSNOW(cl)
# Select relevant time series based on student ID
series_selected<- seq(1500 + student_number_end_digit, 2500, 10)
series<- M3[series_selected]
# Initialize matrices to store error measures for ETS and ARIMA
mape_matrix <- matrix(NA, nrow = length(series), ncol = 2, dimnames = list(NULL, c("ETS", "ARIMA")))
mse_matrix <- matrix(NA, nrow = length(series), ncol = 2, dimnames = list(NULL, c("ETS", "ARIMA")))
mae_matrix <- matrix(NA, nrow = length(series), ncol = 2, dimnames = list(NULL, c("ETS", "ARIMA")))
# Model selection and validation
for (tsi in 1:length(series)) {
print(paste("Processing series", tsi))
data <- series[[tsi]]$x
horizon <- series[[tsi]]$h
y <- head(data, length(data) - horizon)#training data
len <- length(y)
# Get the best model recommended by auto.arima()/ets() for each ts
fit_ets <- ets(y)
fit_arima <- auto.arima(y, method = "CSS")
print(summary(fit_ets))
print(summary(fit_arima))
# Set threshold for cross-validation
if (len > 108) {
ytstart <- len - 36 + 1
} else {
ytstart <- len - 12 + 1
}
origins <- ytstart:len
print(origins)
# Inside your loop for calculating MAPEs
forecast_ets <- forecast(fit_ets, h = horizon)
forecast_arima <- forecast(fit_arima, h = horizon)
print(forecast_ets)
print(forecast_arima)
# Initialize vectors to store MAPEs for ETS and ARIMA
ets_mapes <- numeric(length(origins))
arima_mapes <- numeric(length(origins))
# Parallel computing for cross-validation
MAPEs <- foreach(origin = origins, .combine = 'rbind', .packages = 'forecast') %dopar% {
y_cv <- head(y, origin)
yv_cv <- y[(origin + 1):(origin + horizon)]
cvfit_ets <- ets(y_cv, model = fit_ets$model)
fcs_ets <- forecast(cvfit_ets, h = horizon)$mean
ets_APE <- 100 * (abs(yv_cv - fcs_ets) / abs(yv_cv))
ets_MAPE <- mean(ets_APE, na.rm = TRUE)
cvfit_arima <- Arima(y_cv, model = fit_arima)
fcs_arima <- forecast(cvfit_arima, h = horizon)$mean
arima_APE <- 100 * (abs(yv_cv - fcs_arima) / abs(yv_cv))
arima_MAPE <- mean(arima_APE, na.rm = TRUE)
c(ets_MAPE, arima_MAPE)
}
# Store the mean MAPEs in the matrix
mape_matrix[tsi, "ETS"] <- mean(MAPEs[, 1], na.rm = TRUE)
mape_matrix[tsi, "ARIMA"] <- mean(MAPEs[, 2], na.rm = TRUE)
}
#Part 1 Manual Modelling
#Setting the working directory
setwd("C:/Users/Ritwik Singh/OneDrive - University of Bath/Documents/Semester 1/Business Statistics and Forecasting/Coursework")
#Reading of the CSV data
sanjali_data <- read.csv("MN50751CourseworkData2023.csv")
# Load necessary libraries
library(forecast)
library(smooth)
library(Mcomp)
library(tseries)
library(parallel)
library(foreach)
library(doSNOW)
library(dplyr)
# Set your Student ID last digit
student_number_end_digit <- 30
#Define cluster
cores <- detectCores()
cl <- makeCluster(cores - 1, type = "SOCK")
registerDoSNOW(cl)
# Select relevant time series based on student ID
series_selected<- seq(1500 + student_number_end_digit, 2500, 10)
series<- M3[series_selected]
# Initialize matrices to store error measures for ETS and ARIMA
mape_matrix <- matrix(NA, nrow = length(series), ncol = 2, dimnames = list(NULL, c("ETS", "ARIMA")))
mse_matrix <- matrix(NA, nrow = length(series), ncol = 2, dimnames = list(NULL, c("ETS", "ARIMA")))
mae_matrix <- matrix(NA, nrow = length(series), ncol = 2, dimnames = list(NULL, c("ETS", "ARIMA")))
# Model selection and validation
for (tsi in 1:length(series)) {
print(paste("Processing series", tsi))
data <- series[[tsi]]$x
horizon <- series[[tsi]]$h
y <- head(data, length(data) - horizon)#training data
len <- length(y)
# Get the best model recommended by auto.arima()/ets() for each ts
fit_ets <- ets(y)
fit_arima <- auto.arima(y, method = "CSS")
print(summary(fit_ets))
print(summary(fit_arima))
# Set threshold for cross-validation
if (len > 108) {
ytstart <- len - 36 + 1
} else {
ytstart <- len - 12 + 1
}
origins <- ytstart:len
print(origins)
# Inside your loop for calculating MAPEs
forecast_ets <- forecast(fit_ets, h = horizon)
forecast_arima <- forecast(fit_arima, h = horizon)
print(forecast_ets)
print(forecast_arima)
# Initialize vectors to store MAPEs for ETS and ARIMA
ets_mapes <- numeric(length(origins))
arima_mapes <- numeric(length(origins))
# Parallel computing for cross-validation
MAPEs <- foreach(origin = origins, .combine = 'rbind', .packages = 'forecast') %do% {
y_cv <- head(y, origin)
yv_cv <- y[(origin + 1):(origin + horizon)]
cvfit_ets <- ets(y_cv, model = fit_ets$model)
fcs_ets <- forecast(cvfit_ets, h = horizon)$mean
ets_APE <- 100 * (abs(yv_cv - fcs_ets) / abs(yv_cv))
ets_MAPE <- mean(ets_APE, na.rm = TRUE)
cvfit_arima <- Arima(y_cv, model = fit_arima)
fcs_arima <- forecast(cvfit_arima, h = horizon)$mean
arima_APE <- 100 * (abs(yv_cv - fcs_arima) / abs(yv_cv))
arima_MAPE <- mean(arima_APE, na.rm = TRUE)
c(ets_MAPE, arima_MAPE)
}
# Store the mean MAPEs in the matrix
mape_matrix[tsi, "ETS"] <- mean(MAPEs[, 1], na.rm = TRUE)
mape_matrix[tsi, "ARIMA"] <- mean(MAPEs[, 2], na.rm = TRUE)
}
#Part 1 Manual Modelling
#Setting the working directory
setwd("C:/Users/Ritwik Singh/OneDrive - University of Bath/Documents/Semester 1/Business Statistics and Forecasting/Coursework")
#Reading of the CSV data
sanjali_data <- read.csv("MN50751CourseworkData2023.csv")
# Load necessary libraries
library(forecast)
library(smooth)
library(Mcomp)
library(tseries)
library(parallel)
library(foreach)
library(doSNOW)
library(dplyr)
# Set your Student ID last digit
student_number_end_digit <- 30
#Define cluster
cores <- detectCores()
cl <- makeCluster(cores - 1, type = "SOCK")
registerDoSNOW(cl)
# Select relevant time series based on student ID
series_selected<- seq(1500 + student_number_end_digit, 2500, 10)
series<- M3[series_selected]
# Initialize matrices to store error measures for ETS and ARIMA
mape_matrix <- matrix(NA, nrow = length(series), ncol = 2, dimnames = list(NULL, c("ETS", "ARIMA")))
mse_matrix <- matrix(NA, nrow = length(series), ncol = 2, dimnames = list(NULL, c("ETS", "ARIMA")))
mae_matrix <- matrix(NA, nrow = length(series), ncol = 2, dimnames = list(NULL, c("ETS", "ARIMA")))
# Model selection and validation
for (tsi in 1:length(series)) {
print(paste("Processing series", tsi))
data <- series[[tsi]]$x
horizon <- series[[tsi]]$h
y <- head(data, length(data) - horizon)#training data
len <- length(y)
# Get the best model recommended by auto.arima()/ets() for each ts
fit_ets <- ets(y)
fit_arima <- auto.arima(y, method = "CSS")
print(summary(fit_ets))
print(summary(fit_arima))
# Set threshold for cross-validation
if (len > 108) {
ytstart <- len - 36 + 1
} else {
ytstart <- len - 12 + 1
}
origins <- ytstart:len
print(origins)
# Inside your loop for calculating MAPEs
forecast_ets <- forecast(fit_ets, h = horizon)
forecast_arima <- forecast(fit_arima, h = horizon)
print(forecast_ets)
print(forecast_arima)
# Initialize vectors to store MAPEs for ETS and ARIMA
ets_mapes <- numeric(length(origins))
arima_mapes <- numeric(length(origins))
# Parallel computing for cross-validation
# Inside your loop for calculating MAPEs
for (origin in origins) {
print(paste("Processing origin", origin))
y_cv <- head(y, origin)
yv_cv <- y[(origin + 1):(origin + horizon)]
cvfit_ets <- ets(y_cv, model = fit_ets$model)
fcs_ets <- forecast(cvfit_ets, h = horizon)$mean
ets_APE <- 100 * (abs(yv_cv - fcs_ets) / abs(yv_cv))
ets_MAPE <- mean(ets_APE, na.rm = TRUE)
cvfit_arima <- Arima(y_cv, model = fit_arima)
fcs_arima <- forecast(cvfit_arima, h = horizon)$mean
arima_APE <- 100 * (abs(yv_cv - fcs_arima) / abs(yv_cv))
arima_MAPE <- mean(arima_APE, na.rm = TRUE)
c(ets_MAPE, arima_MAPE)
}
# Store the mean MAPEs in the matrix
mape_matrix[tsi, "ETS"] <- mean(MAPEs[, 1], na.rm = TRUE)
mape_matrix[tsi, "ARIMA"] <- mean(MAPEs[, 2], na.rm = TRUE)
}
